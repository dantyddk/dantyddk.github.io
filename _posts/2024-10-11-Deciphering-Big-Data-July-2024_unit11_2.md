---
layout: post
title: UoE - Deciphering Big Data July 2024 - Individual Reflection & E-Portfolio
subtitle: Design, develop and evaluate solutions for processing datasets and solving complex problems in various environments using relevant programming paradigms.Systematically develop and implement the skills required to be effective member of a development team in a virtual professional environment, adopting real life perspectives on team roles and organisation.
categories: EL-Activities Assignment
tags: [UoE, assignment, essay, Module E-Portfolio Learning Activities]
---
---
## Individual Reflection & E-Portfolio
---

### Final Individual Reflection

This module has provided a comprehensive learning experience that has significantly enhanced my understanding of data management, security, database design, and their application in real-world scenarios. I have engaged in various projects that challenged my skills and encouraged me to develop new approaches to data extraction, cleaning, modelling, and security practices. This reflection focuses on the key learning outcomes, emotional responses, changes in my thinking, and the skills I have gained throughout this journey.

### Learning Outcomes

The projects I worked on were instrumental in expanding my knowledge of database systems, security protocols, and data compliance. Through [Individual Project Executive Summary](https://dantyddk.github.io/el-activities/assignment/2024/10/11/Deciphering-Big-Data-July-2024_unit11_1.html) and [Development Team Project Report](https://dantyddk.github.io/el-activities/assignment/2024/10/10/Deciphering-Big-Data-July-2024_unit6.html). I learned how primary and foreign keys link tables to maintain referential integrity, which is essential for handling large datasets with complex relationships, such as users, ride requests, and dispatches.

The [Backup Procedure](https://dantyddk.github.io/el-activities/wiki/2024/10/11/Deciphering-Big-Data-July-2024_unit11.html) project on the Grandfather-Father-Son (GFS) backup system introduced me to the importance of resource-efficient backup strategies in environments handling large amounts of data. This hierarchical backup method reduces resource intensity while ensuring reliable data recovery. I compared GFS to modern alternatives, such as incremental backups, gaining insights into selecting the right backup strategy based on organizational needs.

Working on the [IBM-QRADAR-Intellas-KAIF Integration Case Study](https://dantyddk.github.io/el-activities/2024/10/11/Deciphering-Big-Data-July-2024-unit10_1.html) deepened my understanding of cybersecurity systems. I explored how QRadar collects and processes security data, while KAIF enhances threat detection through machine learning. The integration of these systems allowed me to understand the importance of inter-process communication (IPC) and seamless data flow between systems to maintain security integrity.

The [API Security Requirements](https://dantyddk.github.io/el-activities/2024/10/11/Deciphering-Big-Data-July-2024-unit10.html) project, developed in collaboration with Matthew Bowyer, focused on securing the integration of the Twitter API using OAuth 2.0, encryption, and rate-limiting mechanisms. This project improved my knowledge of authentication protocols and API key management, I gained a deeper appreciation for the complexity of API-driven systems and the critical role of security in their successful operation.

The [Collaborative Discussion 1](https://dantyddk.github.io/el-activities/discussion/2024/10/10/Deciphering-Big-Data-July-2024.html) & [Collaborative Discussion 2](https://dantyddk.github.io/el-activities/discussion/2024/10/10/Deciphering-Big-Data-July-2024_unit8.html)  allowed me to reflect on the importance of data collection processes and compliance laws. The first discussion emphasized the challenges of collecting large datasets and ensuring data quality during the process, while the second discussion expanded my understanding of GDPR and ICO regulations, particularly how they enforce risk-based security measures.

Practical tasks such as the [Data Build Task](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit7_2.html) and [Normalisation Task](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit7.html) improved my ability to design and implement relational databases. In these projects, I normalized data to ensure efficiency and reduced redundancy, focusing on maintaining referential integrity between related entities. The [Data Management Pipeline Test](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit4_1.html) reinforced best practices in Python programming, which is the experience of implementing real-world database solutions enhanced my technical skills in SQL and database structure optimization

In the [Web Scraping](https://dantyddk.github.io/el-activities/wiki/2024/10/10/Deciphering-Big-Data-July-2024_uni3.html) and [Data Cleaning](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit4.html) tasks, I developed proficiency in data extraction and cleaning techniques using Python libraries such as BeautifulSoup and Requests. These tasks provided me with hands-on experience in preparing raw data for analysis and integrating it into structured formats like JSON.

### Emotional Response and Analysis

Throughout the module, I experienced a wide range of emotions that significantly impacted my learning process. Initially, the complexity of some projects—such as the Database Design and API Security tasks—made me feel overwhelmed. Managing data integrity, ensuring scalability, and implementing security protocols posed challenges that I hadn’t fully anticipated. At times, these difficulties led to moments of frustration, particularly when troubleshooting issues related to performance bottlenecks in database queries or the configuration of API security settings.

However, as I began to apply theoretical concepts to practical scenarios, my confidence grew. Working through the challenges of query optimization and scalability planning allowed me to build a sense of achievement. For example, successfully normalizing the database structure and implementing OAuth 2.0 for API authentication felt like significant accomplishments. These moments of success contributed to my motivation and encouraged me to tackle increasingly complex problems with greater confidence.

Collaborating with my peers in team projects such as the Development Team Project Report and Collaborative Discussions provided me with a sense of satisfaction and team cohesion. Working with others allowed me to benefit from different perspectives and expertise, which helped me improve my own approach to solving problems. While group work sometimes introduced additional challenges—such as coordinating different parts of a project—these challenges ultimately strengthened my ability to work in teams and manage collaborative tasks effectively.

### Learning and Changed Actions

This module has profoundly changed the way I approach data management, security, and system design. One of the most important lessons I learned is the need to embed scalability and security into a project from the beginning. In the Data Management Pipeline Test and Development Team Project, I realized that planning for future growth and ensuring data integrity from the outset is essential to avoid costly modifications later. This realization has shifted my approach to database architecture, ensuring that I now prioritize long-term scalability and security compliance in any project I undertake.

The Backup Procedure project encouraged me to adopt a risk-based approach to data management. I now understand that not every organization requires the same level of backup intensity and that backup strategies should align with the business’s risk tolerance and recovery time objectives. I have applied this principle in my role as a Senior Business Analyst, where I now evaluate backup methods based on both technical needs and business priorities.

Additionally, the hands-on experience with data cleaning and web scraping tasks has enhanced my ability to efficiently extract and clean data from unstructured sources. This new skill has already proven useful in my current role, where I often deal with large volumes of raw data that need to be cleaned and structured before analysis.

### Skills and Knowledge Application

The skills and knowledge I gained throughout this module are directly applicable to my professional role as a Senior Business Analyst. My work on database design has improved my ability to create data systems that are scalable, secure, and efficient. These skills are crucial when managing large datasets that require normalized structures and efficient query processing.

In the area of security, my experience with API authentication, encryption, and data protection protocols has enhanced my capability to assess and implement secure solutions for data-driven applications. I am now confident in developing systems that prioritize data privacy and compliance with regulatory standards, such as GDPR and PCI DSS.

Furthermore, my ability to extract and clean data using Python has been greatly improved through the web scraping and data cleaning tasks. These skills are invaluable when working with large datasets, allowing me to prepare and analyze data more efficiently, particularly in business intelligence and predictive analytics.

### Action Plan

Looking forward, I plan to further develop my knowledge of cloud-based solutions for database scalability and machine learning for advanced data analytics. The projects I worked on in this module sparked an interest in how cloud platforms can provide automatic scaling and how machine learning models can be applied to improve predictive insights. I also aim to refine my skills in data automation through continued use of Python, particularly for large-scale data extraction and integration.

Additionally, I will continue to stay updated on data protection regulations such as GDPR to ensure that I am fully aware of the latest legal requirements when designing data systems. I plan to integrate peer feedback and collaboration into future projects, as the insights gained from team-based activities have proven invaluable in improving my problem-solving abilities.

### Conclusion

In conclusion, this module has been a highly rewarding journey, offering practical and theoretical insights that have enhanced my knowledge of data management, security, and database design. The skills and experiences gained throughout the various projects will continue to inform my professional work, enabling me to design secure, scalable, and efficient systems in the future. I am confident that the lessons learned here will play a crucial role in my ongoing development as a data-driven problem solver.
