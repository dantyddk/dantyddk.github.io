---
layout: post
title: UoE - Deciphering Big Data July 2024 - Individual Reflection & E-Portfolio
subtitle: Design, develop and evaluate solutions for processing datasets and solving complex problems in various environments using relevant programming paradigms.Systematically develop and implement the skills required to be effective member of a development team in a virtual professional environment, adopting real life perspectives on team roles and organisation.
categories: EL-Activities Assignment
tags: [UoE, assignment, essay, Module E-Portfolio Learning Activities]
---
---
##  E-Portfolio
---

### Module E-Portfolio Learning Activities

- [UoE - Deciphering Big Data July 2024 - Individual Reflection & E-Portfolio](https://dantyddk.github.io/el-activities/assignment/2024/10/11/Deciphering-Big-Data-July-2024_unit11_2.html)
- [UoE - Deciphering Big Data July 2024 - Individual Project Executive Summary](https://dantyddk.github.io/el-activities/assignment/2024/10/11/Deciphering-Big-Data-July-2024_unit11_1.html)
- [UoE - Deciphering Big Data July 2024 - Backup Procedure](https://dantyddk.github.io/el-activities/wiki/2024/10/11/Deciphering-Big-Data-July-2024_unit11.html)
- [UoE - Deciphering Big Data July 2024 - IBM-QRADAR-Intellas-KAIF Integration](https://dantyddk.github.io/el-activities/2024/10/11/Deciphering-Big-Data-July-2024-unit10_1.html)
- [UoE - Deciphering Big Data July 2024 - API Security Requirements](https://dantyddk.github.io/el-activities/2024/10/11/Deciphering-Big-Data-July-2024-unit10.html)
- [UoE - Deciphering Big Data July 2024 - Collaborative Discussion 2 - Comparing Compliance Laws](https://dantyddk.github.io/el-activities/discussion/2024/10/10/Deciphering-Big-Data-July-2024_unit8.html)
- [UoE - Deciphering Big Data July 2024 - Data Build Task](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit7_2.html)
- [UoE - Deciphering Big Data July 2024 - Normalisation Task](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit7.html)
- [UoE - Deciphering Big Data July 2024 - Development Team Project Report](https://dantyddk.github.io/el-activities/assignment/2024/10/10/Deciphering-Big-Data-July-2024_unit6.html)
- [UoE - Deciphering Big Data July 2024 - Data Management Pipeline Test](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit4_1.html)
- [UoE - Deciphering Big Data July 2024 - Data Cleaning](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit4.html)
- [UoE - Deciphering Big Data July 2024 - Web Scraping](https://dantyddk.github.io/el-activities/wiki/2024/10/10/Deciphering-Big-Data-July-2024_uni3.html)
- [UoE - Deciphering Big Data July 2024 - Collaborative Discussion 1 - The Data Collection Process](https://dantyddk.github.io/el-activities/discussion/2024/10/10/Deciphering-Big-Data-July-2024.html)

### Reflection On Each Activites

<em>There is an individual reflection section in each assignment; please click on each link to view the details.</em>

#### Individual Project Executive Summary and Development Team Project Report

Through the [Individual Project Executive Summary](https://dantyddk.github.io/el-activities/assignment/2024/10/11/Deciphering-Big-Data-July-2024_unit11_1.html) and [Development Team Project Report](https://dantyddk.github.io/el-activities/assignment/2024/10/10/Deciphering-Big-Data-July-2024_unit6.html), I gained a comprehensive understanding of how to design and implement a relational database for a ride-hailing company. I learned about the critical roles of primary and foreign keys in linking tables to maintain referential integrity, which is essential for managing large datasets with intricate relationships.

For example, understanding how users, ride requests, and dispatches are interconnected allowed me to build a database structure that facilitates efficient querying and data retrieval. This foundational knowledge of relational databases provided me with insights into how to model real-world systems effectively, ensuring data consistency and accuracy across multiple tables.

#### Backup Procedure Project

The [Backup Procedure](https://dantyddk.github.io/el-activities/wiki/2024/10/11/Deciphering-Big-Data-July-2024_unit11.html) project on the Grandfather-Father-Son (GFS) backup system introduced me to the importance of resource-efficient backup strategies in environments handling large volumes of data. I learned that GFS operates on a rotational basis, where daily, weekly, and monthly backups are performed. This hierarchical backup method not only reduces resource intensity but also ensures reliable data recovery.

In comparing GFS to modern alternatives like incremental backups, I gained insights into selecting the right backup strategy based on organizational needs. For instance, I discovered that while GFS is robust for general use, incremental backups can be more efficient in environments that require rapid recovery times and less frequent full backups. This knowledge will be invaluable when evaluating backup solutions for different projects in my professional role.

#### IBM-QRADAR-Intellas-KAIF Integration Case Study

Working on the [IBM-QRADAR-Intellas-KAIF Integration Case Study](https://dantyddk.github.io/el-activities/2024/10/11/Deciphering-Big-Data-July-2024-unit10_1.html) deepened my understanding of cybersecurity systems. I explored how QRadar, as a Security Information and Event Management (SIEM) solution, collects and processes security data, while KAIF enhances threat detection through machine learning techniques.

This project highlighted the significance of inter-process communication (IPC) and the need for seamless data flow between systems to maintain security integrity. For example, understanding how QRadar can integrate data processed by KAIF for real-time threat analysis reinforced the importance of having robust data-sharing mechanisms in cybersecurity frameworks. This experience emphasized the need for proactive security measures in today's interconnected digital landscape.

#### API Security Requirements Project

The [API Security Requirements](https://dantyddk.github.io/el-activities/2024/10/11/Deciphering-Big-Data-July-2024-unit10.html) project, developed in collaboration with Matthew Bowyer, focused on securing the integration of the Twitter API using OAuth 2.0, encryption, and rate-limiting mechanisms. Through this project, I enhanced my knowledge of authentication protocols and API key management, gaining a deeper appreciation for the complexity of API-driven systems.

I learned that the proper implementation of security measures, such as OAuth 2.0, is crucial in preventing unauthorized access and safeguarding sensitive data. This project also underscored the importance of designing APIs with security in mind from the outset, ensuring that data integrity and privacy are prioritized throughout the API's lifecycle.

#### Collaborative Discussions

Participating in [Collaborative Discussion 1](https://dantyddk.github.io/el-activities/discussion/2024/10/10/Deciphering-Big-Data-July-2024.html) and [Collaborative Discussion 2](https://dantyddk.github.io/el-activities/discussion/2024/10/10/Deciphering-Big-Data-July-2024_unit8.html) allowed me to reflect on the importance of data collection processes and compliance laws. The first discussion emphasized the challenges of collecting large datasets and ensuring data quality during the process. I learned about the various methods to ensure accurate and reliable data collection, which is essential for any data-driven project.

In the second discussion, I expanded my understanding of GDPR and ICO regulations, particularly how they enforce risk-based security measures. This knowledge has heightened my awareness of the legal landscape surrounding data privacy and the responsibilities that organizations have in protecting user data. Understanding compliance requirements is critical for designing systems that meet regulatory standards and foster trust among users.

#### Practical Tasks: Data Build Task and Normalisation Task

The [Data Build Task](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit7_2.html) and [Normalisation Task](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit7.html) significantly improved my ability to design and implement relational databases. In these projects, I focused on normalizing data to ensure efficiency and reduce redundancy, which is vital for maintaining referential integrity between related entities.

Normalizing the database structure helped me realize how crucial it is to eliminate duplicate data and create clear relationships between tables. This practical application of normalization principles reinforced my theoretical knowledge and enhanced my skills in SQL and database structure optimization, ensuring that the systems I design are efficient and maintain data integrity.

#### Data Management Pipeline Test

The [Data Management Pipeline Test](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit4_1.html) was particularly enlightening, as it reinforced best practices in Python programming and introduced me to essential concepts in building efficient data management pipelines. I learned how to structure a data management pipeline that encompasses data extraction, transformation, and loading (ETL).

This task taught me the importance of automating data processes to ensure efficiency and accuracy. I gained hands-on experience with techniques for error handling, data validation, and logging, which are critical for maintaining data quality in real-time applications. The skills acquired during this project have enhanced my capability to design robust data pipelines that streamline workflows and improve the overall efficiency of data handling within organizations.

#### Web Scraping and Data Cleaning Tasks

In the [Web Scraping](https://dantyddk.github.io/el-activities/wiki/2024/10/10/Deciphering-Big-Data-July-2024_uni3.html) and [Data Cleaning](https://dantyddk.github.io/el-activities/2024/10/10/Deciphering-Big-Data-July-2024_unit4.html) tasks, I developed proficiency in data extraction and cleaning techniques using Python libraries such as BeautifulSoup and Requests. These tasks provided me with hands-on experience in preparing raw data for analysis and integrating it into structured formats like JSON.

I learned how to effectively scrape data from web pages and clean it to ensure its usability for analysis. This experience is particularly relevant in my current role, where I often encounter large volumes of unstructured data that require transformation into structured formats for reporting and analysis. The skills I gained in data cleaning will enable me to work more efficiently with large datasets and improve the overall quality of my analyses

---
## Individual Reflection
---
### Final Individual Reflection

This module has provided a comprehensive learning experience that significantly enhanced my understanding of data management, security, database design, and their application in real-world scenarios. The projects I engaged with challenged my skills and encouraged me to develop new approaches to data extraction, cleaning, modelling, and security practices. This reflection focuses on the key learning outcomes, emotional responses, changes in my thinking, and the skills I have gained throughout this journey.

### Emotional Response and Analysis

Throughout the module, I experienced a wide range of emotions that significantly impacted my learning process. Initially, the complexity of some projects—such as the Database Design and API Security tasks—made me feel overwhelmed. Managing data integrity, ensuring scalability, and implementing security protocols posed challenges that I hadn’t fully anticipated. These difficulties led to moments of frustration, especially when dealing with performance bottlenecks in database queries or troubleshooting OAuth 2.0 authentication in the API security project.

As I applied theoretical concepts to practical scenarios, my confidence grew. The successful normalization of the ride-hailing database and the implementation of OAuth 2.0 for API authentication were key milestones. These accomplishments gave me a sense of achievement, and I began to feel more motivated to tackle increasingly complex tasks, like query optimization and data flow management in large systems.

Working on team projects such as the Development Team Project Report and Collaborative Discussions was particularly rewarding. Collaborating with my peers fostered a sense of team cohesion and allowed me to benefit from diverse perspectives. This collaborative environment helped me improve my problem-solving abilities, as I learned how to better approach challenges from multiple angles. While group work introduced some challenges, such as coordinating different parts of the project and ensuring seamless integration, these experiences ultimately strengthened my ability to manage collaborative tasks effectively.

### Learning and Changed Actions

This module has fundamentally changed the way I approach data management, security, and system design. One of the most important lessons I learned is the need to embed scalability and security into a project from the very beginning. This was particularly evident in the Data Management Pipeline Test and Development Team Project, where I realized that planning for future growth and ensuring data integrity from the outset is essential to avoid costly modifications later. This realization has shifted my approach to database architecture, ensuring that I now prioritize long-term scalability and security compliance in every project I undertake.

The Backup Procedure project, in particular, encouraged me to adopt a risk-based approach to data management. I now understand that backup strategies must align with the business’s risk tolerance and recovery time objectives. This new perspective has already influenced my work as a Senior Business Analyst, where I now evaluate backup methods not only based on technical factors but also in alignment with the organization's priorities and risk profile.

Hands-on experience with data cleaning and web scraping tasks has further enhanced my ability to efficiently extract and clean data from unstructured sources. This is a critical skill for managing large datasets, and it has already proven useful in my role, where I often work with raw data that needs to be transformed into structured formats for analysis. These new skills have made me more proficient in handling real-world data challenges, particularly in environments where large-scale data extraction and preparation are crucial.

### Skills and Knowledge Application

The skills and knowledge I gained throughout this module are directly applicable to my professional role as a Senior Business Analyst. The work on database design has significantly improved my ability to create data systems that are scalable, secure, and efficient. This is particularly important when managing large datasets that require normalized structures and efficient query processing to ensure that the system can handle complex, high-volume data operations.

In the area of security, my experience with API authentication, encryption, and data protection protocols has enhanced my ability to assess and implement secure solutions for data-driven applications. I now feel confident in developing systems that prioritize data privacy and ensure compliance with regulations such as GDPR and PCI DSS. This has become increasingly important in my role, as I am often tasked with evaluating the security measures of the systems I manage to ensure they meet industry standards.

Additionally, my skills in data extraction and cleaning using Python have greatly improved through tasks like web scraping. These tasks have made me more proficient in handling large datasets, preparing them for analysis, and integrating them into business intelligence workflows. This is a critical capability in today’s data-driven business landscape, and I have already applied these skills in projects that involve complex data analytics and reporting.

### Action Plan

Looking forward, I plan to further develop my knowledge of cloud-based solutions for database scalability and machine learning for advanced data analytics. The projects I worked on during this module sparked an interest in how cloud platforms can enable automatic scaling and how machine learning models can be used to improve predictive insights for businesses. These technologies have the potential to significantly enhance the capabilities of the systems I work with, allowing for more efficient data processing and more accurate forecasts.

I also aim to refine my skills in data automation through the continued use of Python, particularly for large-scale data extraction and integration. Python’s powerful libraries and tools are essential for automating repetitive tasks and ensuring that data flows smoothly through the data pipeline. Mastering automation will allow me to handle even larger datasets and deliver results more quickly and efficiently.

Additionally, I plan to stay updated on data protection regulations such as GDPR to ensure that the systems I design remain compliant with the latest legal requirements. The landscape of data privacy and security is constantly evolving, and staying informed will be crucial to maintaining regulatory compliance in all future projects. I will also continue to integrate peer feedback and collaboration into my work, as the insights gained from team-based activities have proven invaluable in improving my problem-solving abilities and overall project outcomes.

### Conclusion

In conclusion, this module has been a highly rewarding journey, offering both practical and theoretical insights that have significantly enhanced my knowledge of data management, security, and database design. The skills and experiences I have gained through various projects—ranging from data extraction and cleaning to API security and database scalability—will continue to inform my professional work. I am confident that the lessons learned in this module will play a crucial role in my ongoing development as a data-driven problem solver, enabling me to design secure, scalable, and efficient systems that meet the needs of today’s data-driven businesses.
