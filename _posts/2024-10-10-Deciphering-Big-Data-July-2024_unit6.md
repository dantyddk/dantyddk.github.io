---
layout: post
title: UoE - Deciphering Big Data July 2024 - Development Team Project Report
subtitle: Critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques (involving preparing, cleaning, exploring, creating, optimising and evaluating big data) to solve them. Systematically develop and implement the skills required to be effective member of a development team in a virtual professional environment, adopting real life perspectives on team roles and organisation.
categories: EL-Activities Assignment
tags: [UoE, assignment, essay, Module E-Portfolio Learning Activities]
---
---
## Project Report: Database Design for Ride-Hailing Company
---
### Introduction

This report outlines the logical database design for a ride-hailing company, similar to Uber. The primary objective is to create a robust, scalable database management system to efficiently manage the large volumes of data generated by the company's operations. This database will be integral to handling trip records, user information, and service dispatches, supporting both day-to-day operations and strategic decision-making (Connolly & Begg, 2014).

### Logical Database Design

#### Overview

The logical database design phase focuses on developing a data model that accurately represents the ride-hailing system's entities, relationships, and constraints. This model abstracts physical storage concerns, ensuring the database structure is optimized for performance, scalability, and ease of maintenance (Silberschatz, Korth & Sudarshan, 2019).

#### Key Entity Types and Relationships

The logical data model for the ride-hailing company comprises several key entities:
- Requests: Captures the details of ride requests.
    - Primary Key: request_id
    - Attributes: request_created_at, service_type, rider_id, city_id, priority
- Users: Represent the drivers and riders.
    - Primary Key: user_id
    - Attributes: name, email, role, password
- Dispatches: Links ride requests to drivers, managed by dispatchers.
    - Primary Key: dispatch_id
    - Attributes: request_id, driver_id, dispatcher_id, dispatch_time
 
#### Relationships and Integrity Constraints

- One-to-Many (1:M) Relationships:
    - Requests ↔ Dispatches: Each ride request can result in multiple dispatches, but each dispatch is associated with only one request.
    - Dispatches ↔ Users: A dispatch involves one driver, but a driver can handle multiple dispatches.
- Many-to-One (M:1) Relationship:
    - Requests ↔ Users: Each request is made by a single user (rider), but each rider can make multiple requests.
- Integrity Constraints:
    - Entity Integrity: Ensures every table has a unique primary key.
    - Referential Integrity: Maintains valid foreign key relationships between tables, ensuring data consistency across the database.

#### Normalization Process

The normalization process is applied to the database architecture to enhance data integrity and minimize redundancy, adhering to the following principles:
- 1NF - First Normal Form: Affirms that every column in the table consists of singular, indivisible values, ensuring the uniqueness of each record.
- 2NF - Second Normal Form: Eliminates dependencies on only part of the main identifier, ensuring that all columns not part of this key depend entirely on the complete identifier.
- 3NF - Third Normal Form: Removes indirect dependencies, ensuring that attributes outside the primary key depend only on the primary key itself, without relying on other non-primary key columns (Connolly & Begg, 2014; Ponniah, 2010).

#### Entity-Relationship Diagram (ERD)

An ERD was developed to visually represent the logical data model, encompassing entities, primary keys, foreign keys, attributes, and their relationships. The ERD serves as a blueprint for the physical database implementation, ensuring a clear and well-structured database design. MySQL Workbench was used to create the diagram, and forward engineering was employed to generate the script (see Appendix A) (Elmasri & Navathe, 2016).

![image](/assets/images/banners/erd_unit6.jpg)

### Database Build Proposal

#### DBMS Selection

PostgreSQL is recommended as the DBMS for this project due to its open-source nature, advanced features, and strong support for complex queries and large-scale applications. PostgreSQL’s robustness, scalability, and support for JSON data types make it ideal for managing the diverse data requirements of a ride-hailing platform (Silberschatz, Korth & Sudarshan, 2019; Zhang & Pan, 2022).

#### Database Model Design

The proposed database structure includes the following key tables:
- Users: Stores details of both drivers and riders.
    - Key Attributes: user_id, name, email, role, password
- Requests: Records information related to ride requests.
    - Key Attributes: request_id, request_created_at, service_type, rider_id, city_id, priority
- Dispatches: Logs dispatch actions linking requests to drivers.
    - Key Attributes: dispatch_id, request_id, driver_id, dispatcher_id, dispatch_time
 
#### Cloud Hosting and Scalability

To support the system's scalability and availability needs, the database will be hosted on AWS RDS. This cloud-based platform offers automatic backups, failover support, and easy scaling, which are crucial for maintaining service continuity and handling fluctuating workloads typical in a ride-hailing business (Woodie, 2019; IBM, 2010).

#### Security and Access Control

Given the sensitivity of user data, stringent security measures will be implemented, including:
- Encryption: Both data at rest and data in transit will be encrypted using industry-standard protocols.
- Role-Based Access Control (RBAC): Access to the database will be controlled through RBAC, ensuring that only authorized personnel can access or modify sensitive information (Zhang & Pan, 2022).

### Data Management Pipeline

#### Data Capture

The data management pipeline begins with the capture of data from multiple sources, including user applications, GPS devices, and transactional systems. Real-time data ingestion ensures that the database remains updated with minimal delay, facilitating real-time analytics and informed decision-making (Woodie, 2019).

#### Data Cleaning and Preparation

Data cleaning is a critical step in ensuring data accuracy and consistency. Key processes include:
- Deduplication: Eliminating duplicate records to ensure data uniqueness.
- Normalization: Standardizing data formats to maintain consistency across the database.
- Error Correction: Identifying and correcting inaccuracies in the data (Ponniah, 2010; Silberschatz, Korth & Sudarshan, 2019).

These processes are automated through SQL scripts and data processing tools, ensuring efficient and reliable data preparation.

#### Data Validation and Transformation

Post-cleaning, data is validated against business rules to ensure it meets the necessary standards. Data transformation processes are then applied to convert the cleaned data into formats suitable for downstream analytics, reporting, and machine learning tasks.

#### Continuous Improvement

The data management pipeline is designed to evolve with the business. Continuous monitoring and iterative improvements ensure the pipeline remains efficient, accommodating growing data volumes and changing business requirements (Connolly & Begg, 2014).

### Critical Evaluation of the Data Management Pipeline

#### Strengths

- Data Quality: The rigorous data cleaning and validation processes significantly enhance data reliability.
- Scalability: The use of PostgreSQL and cloud hosting ensures the system can scale efficiently as the business grows.
- Security: The implementation of robust security measures protects sensitive user data, maintaining user trust and regulatory compliance.

#### Challenges

- Data Integration: Initial challenges in integrating data from diverse sources were overcome through the development of custom integration scripts.
- Real-Time Processing: Ensuring efficient real-time processing requires ongoing optimization of the data pipeline, particularly as data volumes increase.

#### Recommendations

- Automated Monitoring: Implementing automated monitoring tools can provide real-time insights into the performance of the data pipeline, allowing for proactive issue resolution.
- Regular Audits: Conduct regular audits of the data and pipeline processes to ensure ongoing compliance with data governance standards and to identify opportunities for further improvement.

#### Conclusion

The logical database design and build proposal outlined in this report provide a solid foundation for the ride-hailing company’s data management needs. With PostgreSQL as the DBMS, cloud hosting for scalability, and a carefully designed data management pipeline, the database system is well-equipped to handle current operations while supporting future growth. Continuous monitoring and iterative improvements will ensure the system remains robust, secure, and adaptable in a dynamic business environment (Silberschatz, Korth & Sudarshan, 2019; Connolly & Begg, 2014).

### References

- Quvvatov, B. (2024). <em>SQL Databases and Big Data Analytics: Navigating the Data Management Landscape</em>. Available at: http://www.econferences.ru/index.php/dptms/article/view/11708 [Accessed 21 August 2024].
- Silberschatz, A., Korth, H.F., & Sudarshan, S. (2019). <em>Database System Concepts</em>. 7th edn. McGraw-Hill Education.
- Connolly, T., & Begg, C. (2014). <em>Database Systems: A Practical Approach to Design, Implementation, and Management</em>. Pearson Education Limited.
- Ponniah, P. (2010). <em>Data Modeling Fundamentals: A Practical Guide for IT Professionals</em>. Wiley.
- Elmasri, R., & Navathe, S.B. (2016). <em>Fundamentals of Database Systems</em>. 7th edn. Pearson.
- IBM (2010). <em>What is a Database Management System?</em> [online]. Available at: https://www.ibm.com/docs/en/zos-basic-skills?topic=zos-what-is-database-management-system [Accessed 22 August 2024].
- Vassiliadis, P.A., Simitsis, A., & Skiadopoulos, S. (2002). <em>Conceptual Modeling for ETL Processes. In 21st International Conference on Conceptual Modeling</em>. Springer, pp. 14-21.
- Woodie, A. (2019). <em>Data Pipeline Automation: The Next Step Forward in DataOps</em>. Available at: https://www.datanami.com/2019/04/24/data-pipeline-automation-the-next-step-forward-in-dataops/ [Accessed 21 August 2024].
- Zhang, Y., & Pan, F. (2022). <em>Design and Implementation of a New Intelligent Warehouse Management System Based on MySQL Database Technology</em>. Available at: https://www.informatica.si/index.php/informatica/article/view/3348 [Accessed 21 August 2024].

---
## Tutor Feedback
---

The report shows very good knowledge and understanding of module topics, incorporated within a practical application as required. The application is for an UBER like company. The content and structure of the report are good, the word count is within limit, and the English language is sound. The report redresses all requirements and articulates between the various sections. The critical discussions are limited in depth, including in the critical evaluation section. The report includes nine references, but these are not cited within the sections. Please note that critical discussions are an important component of an evaluation and requires better attention. There are two diagrams floated within the report, but these require numbers and captions and more importantly further explanations within the text. The SQL code in the appendix is not a requirement.

---
## Individual Reflecion
---

### Reflection

The Database Design for Ride-Hailing Company project was a highly educational experience, allowing me to apply database design concepts in a real-world scenario. Throughout the project, I worked closely with my team to ensure a cohesive, scalable solution was developed for managing the data generated by a ride-hailing platform.

### Role in the Team

My primary responsibility in this project was focused on the Data Management Pipeline and the Evaluation of the Data Management Pipeline sections. I contributed by designing a pipeline that ensured efficient data capture, cleaning, validation, and transformation processes. This involved understanding the real-time nature of ride-hailing data and developing a robust system that could handle continuous data ingestion while maintaining data integrity.

As part of the team, I contributed significantly to the logical database design and the development of the Entity-Relationship Diagram (ERD). Working on the ERD deepened my understanding of the relationship between different entities, such as users, requests, and dispatches, and how to maintain referential integrity through foreign key constraints. My contributions also involved applying normalization techniques (1NF, 2NF, and 3NF) to ensure the database was optimized for data integrity and minimal redundancy.

Additionally, I took on the responsibility of combining all team members' contributions into a single cohesive report. This required attention to detail, as I ensured consistency in the writing style, proper transitions between sections, and alignment of the content with the project’s objectives. My role in writing the final version also involved making sure that all key points were articulated clearly and that the report addressed all necessary requirements.

### Learning Through Application

The project provided me with an opportunity to apply my knowledge of data management pipelines in a practical setting. Designing the pipeline involved automating processes like data cleaning, deduplication, and error correction using SQL scripts and other tools. This hands-on experience helped me understand how crucial it is to have a well-structured pipeline that ensures data reliability, especially in a system that deals with real-time ride requests and dispatches.

The evaluation section required critical thinking to assess the strengths and challenges of the pipeline. I learned to identify potential bottlenecks, such as ensuring real-time data processing while maintaining performance, and how to propose improvements like automated monitoring and regular audits.

### Challenges and Overcoming Them

One of the main challenges was coordinating the integration of different sections contributed by team members. Since I was responsible for compiling the final report, I had to ensure that each section was aligned, consistent, and free of overlap. This involved thorough editing and revisions to make sure that the document flowed smoothly.

Another challenge was ensuring the data pipeline met the needs of the business. Developing a pipeline for a ride-hailing platform that operates in real-time required careful planning and optimization to handle the large volumes of data generated from various sources. To overcome this, I focused on designing scalable processes and integrating feedback from the team to improve the pipeline’s efficiency.

### Incorporating Feedback

The feedback from our tutor highlighted the need for deeper critical discussions in the evaluation section. I now realize that, while the technical aspects were well-covered, I could have expanded on the implications of certain design decisions. For instance, comparing different pipeline tools or discussing the trade-offs between PostgreSQL and MySQL would have strengthened the critical evaluation.

Additionally, I will focus more on properly integrating citations throughout the report in the future. Although the reference list was well-constructed, the absence of in-text citations made it difficult to connect the theory with the application.

### Conclusion

Overall, this project allowed me to refine my skills in database design and data management pipelines. I also developed my ability to combine and synthesize different sections of a team project into a coherent final report. Moving forward, I will apply the lessons learned about critical evaluation and documentation to improve my contributions to future projects.


